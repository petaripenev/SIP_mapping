import pandas as pd
import os

configfile: "config/config.yaml"

df_sample = pd.read_csv(config["samples"], sep="\t")
df_sample = df_sample.fillna("")
df_sample[df_sample.columns] = df_sample.apply(lambda x: x.str.strip())
fractions = []

for header_col in df_sample:
    if header_col.startswith("fraction_"):
        fraction = header_col.split("_")[1]
        isotope = header_col.split("_")[2]
        fr_rev = f"fraction_{fraction}_{isotope}_rv"
        df_sample[fr_rev] = df_sample[header_col].apply(lambda x: x.replace(".PE.1.fastq.gz", ".PE.2.fastq.gz"))
        if fraction not in fractions:
            fractions.append(int(fraction))
    for inc_time in config["incubation_times"]:
        if header_col.startswith(f"t{inc_time}_"):
            fwrv = header_col.split("_")[1]
            df_sample[f"t{inc_time}_rv"] = df_sample[header_col].apply(lambda x: x.replace(".PE.1.fastq.gz", ".PE.2.fastq.gz"))

#print(df_sample.to_markdown())
fraction_loop_regex = f"[{min(fractions)}-{max(fractions)}]+"
sample_to_info = df_sample.set_index("sample_name").to_dict("index")

#Check that all samples have the same scaffolds_path
if config["single_scaffold_file"] == 't':
    SINGLE_SCAFFOLD_ERROR = f"Option `single_scaffold_file` is set to {config['single_scaffold_file']} but must be either 't' or 'f'!"
    if len(df_sample["scaffolds_path"].unique()) > 1:
        raise IOError("Multiple scaffolds_path found in samples.tsv. \
        When setting `single_scaffold_file` to true, please have the same scaffolds_path for all samples!")


output_list = [
    expand("results/scaffolds/{sample}_scaffold.fa", 
            sample=sample_to_info.keys(),) if config["single_scaffold_file"] == 'f' \
     else f"results/scaffolds/{config['output_prefix']}_scaffold.fa" if config["single_scaffold_file"] == 't' \
     else SINGLE_SCAFFOLD_ERROR,
    expand("results/reads/t{inc_time}/{sample}_t{inc_time}_fw.fq.gz", 
            sample=sample_to_info.keys(),
            inc_time=config["incubation_times"],
    ),
    #"results/merged_scaffold/ref_index",
    expand("results/{sample}/ref_index/{sample}", 
            sample=sample_to_info.keys(),) if config["single_scaffold_file"] == 'f' \
      else f"results/{config['output_prefix']}_ref_index" if config["single_scaffold_file"] == 't' \
      else SINGLE_SCAFFOLD_ERROR,
    # expand("results/maps/{n}/{sample}_{n}_{isotope}_mapped.sort.bam", 
    #         sample=sample_to_info.keys(),
    #         n=sorted(fractions),
    #         isotope=config["isotopes"],
    # ),
    expand("results/counts/{sample}_{cov_calc_method}_table.tsv",
        sample=sample_to_info.keys(), cov_calc_method=['tpm','reads_per_base']) if config["single_scaffold_file"] == 'f' \
      else expand("results/counts/{output_prefix}/{cov_calc_method}_table.tsv", 
        cov_calc_method=['tpm','reads_per_base'], output_prefix=config['output_prefix']) if config["single_scaffold_file"] == 't' \
      else SINGLE_SCAFFOLD_ERROR,
    # expand("results/counts/{sample}_tpm_table.tsv",
    #     sample=sample_to_info.keys(),) if config["single_scaffold_file"] == 'f' \
    #   else "results/counts/tpm_table.tsv" if config["single_scaffold_file"] == 't' \
    #   else SINGLE_SCAFFOLD_ERROR,
    expand("results/maps/{sample}/{sample}_merged_mapped.sort.bam",
        sample=sample_to_info.keys(),) if config["single_scaffold_file"] == 'f' \
      else expand(f"results/maps/{config['output_prefix']}/"+"{sample}_merged_mapped.sort.bam",
        sample=sample_to_info.keys(),) if config["single_scaffold_file"] == 't' \
      else SINGLE_SCAFFOLD_ERROR,

]

if config['single_scaffold_file'] == 'f':
    output_list.append("results/counts/merged_tpm_table.tsv")

rule all:
    input:
        output_list

localrules: link_input_files, link_fr_reads, merge_count_files

rule link_scaffolds_path:
    input:
        scaffolds_path = lambda wildcards: sample_to_info[wildcards.sample]["scaffolds_path"] if config["single_scaffold_file"] == 'f' \
                    else str(df_sample["scaffolds_path"].unique()[0]) if config["single_scaffold_file"] == 't' \
                    else SINGLE_SCAFFOLD_ERROR,
    output:
        linked_scaffolds_path = "results/scaffolds/{sample}_scaffold.fa" if config["single_scaffold_file"] == 'f' \
                    else f"results/scaffolds/{config['output_prefix']}_scaffold.fa" if config["single_scaffold_file"] == 't' \
                    else SINGLE_SCAFFOLD_ERROR,
    group:
        1
    shell:
        """
        ln -s {input.scaffolds_path} {output.linked_scaffolds_path}
        """

rule link_input_files:
    input:
        unfr_fw = lambda wildcards: sample_to_info[wildcards.sample][f"t{wildcards.inc_time}_fw"],
        unfr_rv = lambda wildcards: sample_to_info[wildcards.sample][f"t{wildcards.inc_time}_rv"],
        #bins_path = lambda wildcards: sample_to_info[wildcards.sample]["contig_to_bin"],
    output:
        unfr_fw_link = "results/reads/t{inc_time}/{sample}_t{inc_time}_fw.fq.gz",
        unfr_rv_link = "results/reads/t{inc_time}/{sample}_t{inc_time}_rv.fq.gz",
        #linked_bins_path = "results/dastool_bins/{sample}.dastool.contig_to_bin.tsv",
    wildcard_constraints:
        inc_time=config["incubation_times"],
    group:
        1
    shell:
        """
        ln -s {input.unfr_fw} {output.unfr_fw_link}
        ln -s {input.unfr_rv} {output.unfr_rv_link}
        """
        #ln -s {input.bins_path} {output.linked_bins_path}

rule link_fr_reads:
    input:
        fw_reads = lambda wildcards: sample_to_info[wildcards.sample][f"fraction_{wildcards.n}_{wildcards.isotope}_fw"],
        rv_reads = lambda wildcards: sample_to_info[wildcards.sample][f"fraction_{wildcards.n}_{wildcards.isotope}_rv"],
    output:
        fw_reads_link = "results/reads/{n}/{sample}_{n}_{isotope}_fw.fq.gz",
        rv_reads_link = "results/reads/{n}/{sample}_{n}_{isotope}_rv.fq.gz",
    wildcard_constraints:
        n=fraction_loop_regex,
        isotope=config["isotope_regex"],
    group:
        1
    shell:
        """
        ln -s {input.fw_reads} {output.fw_reads_link}
        ln -s {input.rv_reads} {output.rv_reads_link}
        """

def get_fw_reads(wcs):
    return sample_to_info[wcs.sample][f"fraction_{wcs.n}_{wcs.isotope}_fw"]
def get_rv_reads(wcs):
    return sample_to_info[wcs.sample][f"fraction_{wcs.n}_{wcs.isotope}_rv"]

if config["mapper"] == "bbmap":
    include: "rules/map_bbmap.smk"
elif config["mapper"] == "bowtie2":
    include: "rules/map_bowtie2.smk"
else:
    raise ValueError(f"Mapper {config['mapper']} not supported!")

rule get_count_table:
    input:
        unfr_map = expand("results/maps/t{inc_time}/{{sample}}-v-{sample_map}_t{inc_time}_mapped.sort.bam",
                        sample_map=sample_to_info.keys(), inc_time=config["incubation_times"]) if config["single_scaffold_file"] == 'f' \
            else expand("results/maps/{output_prefix}/t{inc_time}/{sample_map}_t{inc_time}_mapped.sort.bam",
                        sample_map=sample_to_info.keys(), output_prefix=config['output_prefix'], inc_time=config["incubation_times"]) if config["single_scaffold_file"] == 't' else SINGLE_SCAFFOLD_ERROR,
        fr_maps = sorted(list(set(expand("results/maps/{n}/{{sample}}-v-{sample_map}_{n}_{isotope}_mapped.sort.bam",
                        sample_map=sample_to_info.keys(), n=sorted(fractions), isotope=config["isotopes"])))) if config["single_scaffold_file"] == 'f' \
             else sorted(list(set(expand("results/maps/{output_prefix}/{n}/{sample_map}_{n}_{isotope}_mapped.sort.bam",
                        sample_map=sample_to_info.keys(), n=sorted(fractions), isotope=config["isotopes"], output_prefix=config['output_prefix'])))) if config["single_scaffold_file"] == 't' \
             else SINGLE_SCAFFOLD_ERROR,
    output:
        count_table = "results/counts/{sample}_{cov_calc_method}_table.tsv" if config["single_scaffold_file"] == 'f' \
                 else "results/counts/{output_prefix}/{cov_calc_method}_table.tsv" if config["single_scaffold_file"] == 't' else SINGLE_SCAFFOLD_ERROR,
    log:
        "logs/counts/{sample}_{cov_calc_method}_table.log" if config["single_scaffold_file"] == 'f' \
        else "logs/counts/{output_prefix}/{cov_calc_method}_table.log" if config["single_scaffold_file"] == 't' else SINGLE_SCAFFOLD_ERROR,
    params:
        min_read_perc_id=config["coverm_perc_id"],
    threads: config["max_threads"]
    benchmark:
        "benchmarks/counts/{sample}_{cov_calc_method}_table.tsv" if config["single_scaffold_file"] == 'f' \
        else "benchmarks/counts/{output_prefix}/{cov_calc_method}_table.tsv" if config["single_scaffold_file"] == 't' else SINGLE_SCAFFOLD_ERROR,
    wildcard_constraints:
        cov_calc_method="reads_per_base|tpm",
        output_prefix=config["output_prefix"],
    shell:
        """
        coverm contig \
        --bam-files {input.unfr_map} {input.fr_maps} \
        -m {wildcards.cov_calc_method} \
        -o {output.count_table} \
        -t {threads} \
        --min-read-percent-identity {params.min_read_perc_id} \
        > {log}
        """

rule merge_bams:
    input:
        unfr_map = expand("results/maps/t{inc_time}/{{sample}}-v-{sample_map}_t{inc_time}_mapped.sort.bam",
                        sample_map=sample_to_info.keys(), inc_time=config["incubation_times"]) if config["single_scaffold_file"] == 'f' \
            else expand("results/maps/{output_prefix}/t{inc_time}/{{sample}}_t{inc_time}_mapped.sort.bam",
                        sample=sample_to_info.keys(), output_prefix=config['output_prefix'], inc_time=config["incubation_times"]) if config["single_scaffold_file"] == 't' else SINGLE_SCAFFOLD_ERROR,
        fr_maps = sorted(list(set(expand("results/maps/{n}/{{sample}}-v-{sample_map}_{n}_{isotope}_mapped.sort.bam",
                        sample_map=sample_to_info.keys(), n=sorted(fractions), isotope=config["isotopes"])))) if config["single_scaffold_file"] == 'f' \
             else sorted(list(set(expand("results/maps/{output_prefix}/{n}/{{sample}}_{n}_{isotope}_mapped.sort.bam",
                        sample=sample_to_info.keys(), n=sorted(fractions), isotope=config["isotopes"], output_prefix=config['output_prefix'])))) if config["single_scaffold_file"] == 't' \
             else SINGLE_SCAFFOLD_ERROR,
    output:
        merged_map = "results/maps/{sample}/{sample}_merged_mapped.sort.bam" if config["single_scaffold_file"] == 'f' \
                 else "results/maps/{output_prefix}/{sample}_merged_mapped.sort.bam" if config["single_scaffold_file"] == 't' else SINGLE_SCAFFOLD_ERROR,
    log: "logs/maps/{sample}_merged_mapped.log" if config["single_scaffold_file"] == 'f' \
        else "logs/maps/{output_prefix}/{sample}_merged_mapped.log" if config["single_scaffold_file"] == 't' else SINGLE_SCAFFOLD_ERROR,
    threads: config["max_threads"]
    wildcard_constraints:
        output_prefix=config["output_prefix"],
    shell:
        """
        samtools merge -r -@ {threads} {output.merged_map} {input.unfr_map} {input.fr_maps}
        """

if config['single_scaffold_file'] == 'f':
    rule merge_count_files:
        input:
            raw_counts = expand("results/counts/{sample}_count_table.tsv",
                                sample=sample_to_info.keys(),
                                ),
            tpm_counts = expand("results/counts/{sample}_tpm_table.tsv",
                                sample=sample_to_info.keys(),
                                ),
        output:
            count_headers = "results/counts/rawcount_headers.tsv",
            tpm_headers = "results/counts/tpm_headers.tsv",
            counts = "results/counts/merged_count_table.tsv",
            tpms = "results/counts/merged_tpm_table.tsv",
        shell:
            """
            for file in {input.raw_counts}; do
                sed -E '1 s/WaterYear_.{{3}}_.{{2}}_full_coass-v-//g' $file | sed -E '1 s/\sRead Count\\t/\\t/g' | head -n1 >> {output.count_headers}
            done
            &&
            for file in {input.tpm_counts}; do
                sed -E '1 s/WaterYear_.{{3}}_.{{2}}_full_coass-v-//g' $file | sed -E '1 s/\sRead Count\\t/\\t/g' | head -n1 >> {output.tpm_headers}
            done
            &&
            if [ $(sort {output.count_headers} | uniq -d | wc -l) -gt 1 ]; then 
                echo "Headers are differring! Must manually merge files!"; 
            else 
                sort {output.count_headers} | uniq -d > {output.counts}
                tail -q -n +2 {input.raw_counts} >> {output.counts}
            fi
            &&
            if [ $(sort {output.tpm_headers} | uniq -d | wc -l) -gt 1 ]; then 
                echo "TPM Headers are differring! Must manually merge files!"; 
            else 
                sort {output.tpm_headers} | uniq -d > {output.tpms}
                tail -q -n +2 {input.tpm_counts} >> {output.tpms}
            fi
            """